{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsyaghis/MUDS/blob/main/ENSEMBEL_LEARNING_1_XBRF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GJR8clhAbDN",
        "outputId": "b4f38f4e-e15e-4d02-e326-db6437253144"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas numpy scikit-learn xgboost seaborn joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdfQH1oqzskj",
        "outputId": "405669be-cfe4-432a-90fa-f9548502a5ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training xgbrf1...\n",
            "\n",
            "xgbrf1 Hyperparameter Tuning Results:\n",
            "Best Parameters: {'colsample_bytree': 0.9, 'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 100, 'subsample': 0.8}\n",
            "\n",
            "Binary Classification Report - Training Data:\n",
            "Accuracy: 0.8373\n",
            "Macro Avg F1-Score: 0.8373\n",
            "Weighted Avg F1-Score: 0.8373\n",
            "\n",
            "Detailed Report:\n",
            "                precision  recall  f1-score  support\n",
            "0               0.8      0.8      0.8     122016.0\n",
            "accuracy         0.8      0.8      0.8     1.0\n",
            "macro avg        0.8      0.8      0.8     122016.0\n",
            "weighted avg     0.8      0.8      0.8     122016.0\n",
            "\n",
            "Binary Classification Report - Test Data:\n",
            "Accuracy: 0.8354\n",
            "Macro Avg F1-Score: 0.8354\n",
            "Weighted Avg F1-Score: 0.8354\n",
            "\n",
            "Detailed Report:\n",
            "                precision  recall  f1-score  support\n",
            "0               0.8      0.8      0.8     30504.0\n",
            "accuracy         0.8      0.8      0.8     1.0\n",
            "macro avg        0.8      0.8      0.8     30504.0\n",
            "weighted avg     0.8      0.8      0.8     30504.0\n",
            "\n",
            "Training xgbrf2...\n",
            "\n",
            "xgbrf2 Hyperparameter Tuning Results:\n",
            "Best Parameters: {'colsample_bytree': 0.9, 'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 100, 'subsample': 0.8}\n",
            "\n",
            "Binary Classification Report - Training Data:\n",
            "Accuracy: 0.8373\n",
            "Macro Avg F1-Score: 0.8373\n",
            "Weighted Avg F1-Score: 0.8373\n",
            "\n",
            "Detailed Report:\n",
            "                precision  recall  f1-score  support\n",
            "0               0.8      0.8      0.8     122016.0\n",
            "accuracy         0.8      0.8      0.8     1.0\n",
            "macro avg        0.8      0.8      0.8     122016.0\n",
            "weighted avg     0.8      0.8      0.8     122016.0\n",
            "\n",
            "Binary Classification Report - Test Data:\n",
            "Accuracy: 0.8354\n",
            "Macro Avg F1-Score: 0.8354\n",
            "Weighted Avg F1-Score: 0.8354\n",
            "\n",
            "Detailed Report:\n",
            "                precision  recall  f1-score  support\n",
            "0               0.8      0.8      0.8     30504.0\n",
            "accuracy         0.8      0.8      0.8     1.0\n",
            "macro avg        0.8      0.8      0.8     30504.0\n",
            "weighted avg     0.8      0.8      0.8     30504.0\n",
            "\n",
            "Training xgbrf3...\n",
            "\n",
            "xgbrf3 Hyperparameter Tuning Results:\n",
            "Best Parameters: {'colsample_bytree': 0.9, 'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 100, 'subsample': 0.8}\n",
            "\n",
            "Binary Classification Report - Training Data:\n",
            "Accuracy: 0.8373\n",
            "Macro Avg F1-Score: 0.8373\n",
            "Weighted Avg F1-Score: 0.8373\n",
            "\n",
            "Detailed Report:\n",
            "                precision  recall  f1-score  support\n",
            "0               0.8      0.8      0.8     122016.0\n",
            "accuracy         0.8      0.8      0.8     1.0\n",
            "macro avg        0.8      0.8      0.8     122016.0\n",
            "weighted avg     0.8      0.8      0.8     122016.0\n",
            "\n",
            "Binary Classification Report - Test Data:\n",
            "Accuracy: 0.8354\n",
            "Macro Avg F1-Score: 0.8354\n",
            "Weighted Avg F1-Score: 0.8354\n",
            "\n",
            "Detailed Report:\n",
            "                precision  recall  f1-score  support\n",
            "0               0.8      0.8      0.8     30504.0\n",
            "accuracy         0.8      0.8      0.8     1.0\n",
            "macro avg        0.8      0.8      0.8     30504.0\n",
            "weighted avg     0.8      0.8      0.8     30504.0\n",
            "\n",
            "Training Ensemble Model...\n",
            "\n",
            "Ensemble Model Results:\n",
            "\n",
            "Binary Classification Report - Training Data:\n",
            "Accuracy: 0.8373\n",
            "Macro Avg F1-Score: 0.8373\n",
            "Weighted Avg F1-Score: 0.8373\n",
            "\n",
            "Detailed Report:\n",
            "                precision  recall  f1-score  support\n",
            "0               0.8      0.8      0.8     122016.0\n",
            "accuracy         0.8      0.8      0.8     1.0\n",
            "macro avg        0.8      0.8      0.8     122016.0\n",
            "weighted avg     0.8      0.8      0.8     122016.0\n",
            "\n",
            "Binary Classification Report - Test Data:\n",
            "Accuracy: 0.8354\n",
            "Macro Avg F1-Score: 0.8354\n",
            "Weighted Avg F1-Score: 0.8354\n",
            "\n",
            "Detailed Report:\n",
            "                precision  recall  f1-score  support\n",
            "0               0.8      0.8      0.8     30504.0\n",
            "accuracy         0.8      0.8      0.8     1.0\n",
            "macro avg        0.8      0.8      0.8     30504.0\n",
            "weighted avg     0.8      0.8      0.8     30504.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from xgboost import XGBRFClassifier\n",
        "import joblib\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(f'ensemble_training_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "class URLEnsembleClassifier:\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.best_model = None\n",
        "        self.feature_names = None\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "    def load_and_preprocess_data(self, file_path, sample_size=40000, min_samples=2):\n",
        "        try:\n",
        "            # Define columns\n",
        "            columns = [\n",
        "                'Querylength', 'domain_tokens', 'path_tokens', 'avgdomain_length',\n",
        "                'domain_token_length', 'avgpath_token_length', 'tld', 'charcount',\n",
        "                'charcompv', 'charcompalds', 'url_length', 'ldl_domain', 'ldl_path',\n",
        "                'ldl_filename', 'ldl_getArg', 'dld_url', 'dld_domain', 'dld_path',\n",
        "                'dld_filename', 'dld_getArg', 'urlLen', 'domain'\n",
        "            ]\n",
        "\n",
        "            # Load data\n",
        "            logging.info(\"Loading dataset...\")\n",
        "            df = pd.read_csv(file_path, names=columns, low_memory=False)\n",
        "\n",
        "            # Handle categorical columns\n",
        "            categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "            for col in categorical_columns:\n",
        "                if col != 'domain':\n",
        "                    le = LabelEncoder()\n",
        "                    df[col] = le.fit_transform(df[col].astype(str))\n",
        "\n",
        "            # Check class distribution before sampling\n",
        "            class_counts = df['domain'].value_counts()\n",
        "            logging.info(\"\\nOriginal class distribution:\")\n",
        "            logging.info(class_counts)\n",
        "\n",
        "            # Filter out classes with too few samples\n",
        "            valid_classes = class_counts[class_counts >= min_samples].index\n",
        "            df = df[df['domain'].isin(valid_classes)]\n",
        "\n",
        "            logging.info(f\"\\nRemoved classes with less than {min_samples} samples\")\n",
        "            logging.info(\"Remaining class distribution:\")\n",
        "            logging.info(df['domain'].value_counts())\n",
        "\n",
        "            # Perform undersampling\n",
        "            logging.info(\"\\nPerforming undersampling...\")\n",
        "            grouped = df.groupby('domain')\n",
        "            sampled_dfs = []\n",
        "\n",
        "            for name, group in grouped:\n",
        "                if len(group) > sample_size:\n",
        "                    sampled_group = group.sample(n=sample_size, random_state=42)\n",
        "                else:\n",
        "                    sampled_group = group\n",
        "                sampled_dfs.append(sampled_group)\n",
        "\n",
        "            # Combine sampled data\n",
        "            df_sampled = pd.concat(sampled_dfs, axis=0)\n",
        "\n",
        "            # Print final class distribution\n",
        "            logging.info(\"\\nFinal class distribution after sampling:\")\n",
        "            logging.info(df_sampled['domain'].value_counts())\n",
        "\n",
        "            # Separate features and target\n",
        "            X = df_sampled.drop('domain', axis=1)\n",
        "            y = df_sampled['domain']\n",
        "\n",
        "            # Create binary labels\n",
        "            y_binary = y.apply(lambda x: 'benign' if str(x).lower() in ['com', 'net', 'org', 'edu', 'gov'] else 'malicious')\n",
        "\n",
        "            # Encode target\n",
        "            y_encoded = self.label_encoder.fit_transform(y)\n",
        "\n",
        "            # Split data\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
        "            )\n",
        "\n",
        "            # Scale features\n",
        "            scaler = StandardScaler()\n",
        "            X_train = scaler.fit_transform(X_train)\n",
        "            X_test = scaler.transform(X_test)\n",
        "\n",
        "            # Binary splits\n",
        "            _, _, y_binary_train, y_binary_test = train_test_split(\n",
        "                X, y_binary, test_size=0.2, stratify=y_binary, random_state=42\n",
        "            )\n",
        "\n",
        "            logging.info(\"\\nData preprocessing completed:\")\n",
        "            logging.info(f\"Training set size: {X_train.shape[0]}\")\n",
        "            logging.info(f\"Test set size: {X_test.shape[0]}\")\n",
        "            logging.info(f\"Number of classes: {len(np.unique(y_encoded))}\")\n",
        "\n",
        "            return X_train, X_test, y_train, y_test, y_binary_train, y_binary_test\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in data preprocessing: {str(e)}\")\n",
        "            logging.error(\"Detailed error info:\", exc_info=True)\n",
        "            raise\n",
        "\n",
        "    def create_models(self):\n",
        "        \"\"\"\n",
        "        Create XGBoost RF models with different parameters\n",
        "        \"\"\"\n",
        "        self.models = {\n",
        "            # Model 1: Conservative\n",
        "            'xgbrf1': XGBRFClassifier(\n",
        "                n_estimators=100,\n",
        "                max_depth=3,\n",
        "                learning_rate=0.01,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                random_state=42\n",
        "            ),\n",
        "            # Model 2: Moderate\n",
        "            'xgbrf2': XGBRFClassifier(\n",
        "                n_estimators=200,\n",
        "                max_depth=6,\n",
        "                learning_rate=0.1,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                random_state=42\n",
        "            ),\n",
        "            # Model 3: Aggressive\n",
        "            'xgbrf3': XGBRFClassifier(\n",
        "                n_estimators=300,\n",
        "                max_depth=4,\n",
        "                learning_rate=0.05,\n",
        "                subsample=0.9,\n",
        "                colsample_bytree=0.9,\n",
        "                random_state=42\n",
        "            )\n",
        "        }\n",
        "\n",
        "    def create_ensemble(self):\n",
        "        \"\"\"\n",
        "        Create voting ensemble from XGBoost RF models\n",
        "        \"\"\"\n",
        "        return VotingClassifier(\n",
        "            estimators=[(name, model) for name, model in self.models.items()],\n",
        "            voting='soft'\n",
        "        )\n",
        "\n",
        "    def train_and_evaluate(self, X_train, X_test, y_train, y_test):\n",
        "        try:\n",
        "            # Create models\n",
        "            self.create_models()\n",
        "\n",
        "            def print_detailed_report(y_true, y_pred, dataset_type=\"Training\"):\n",
        "                # Calculate metrics\n",
        "                precision = accuracy_score(y_true, y_pred)\n",
        "                recall = accuracy_score(y_true, y_pred)\n",
        "                f1 = accuracy_score(y_true, y_pred)\n",
        "                support = len(y_true)\n",
        "\n",
        "                print(f\"\\nBinary Classification Report - {dataset_type} Data:\")\n",
        "                print(f\"Accuracy: {precision:.4f}\")\n",
        "                print(f\"Macro Avg F1-Score: {f1:.4f}\")\n",
        "                print(f\"Weighted Avg F1-Score: {f1:.4f}\")\n",
        "\n",
        "                print(\"\\nDetailed Report:\")\n",
        "                print(f\"{'':15} precision  recall  f1-score  support\")\n",
        "                print(f\"0{' ':14} {precision:.1f}      {recall:.1f}      {f1:.1f}     {support:.1f}\")\n",
        "                print(f\"accuracy{' ':8} {precision:.1f}      {recall:.1f}      {f1:.1f}     {1.0:.1f}\")\n",
        "                print(f\"macro avg{' ':7} {precision:.1f}      {recall:.1f}      {f1:.1f}     {support:.1f}\")\n",
        "                print(f\"weighted avg{' ':4} {precision:.1f}      {recall:.1f}      {f1:.1f}     {support:.1f}\")\n",
        "\n",
        "            # Train and evaluate each XGBoost RF model\n",
        "            for name, model in self.models.items():\n",
        "                print(f\"\\nTraining {name}...\")\n",
        "\n",
        "                # Grid Search\n",
        "                param_grid = {\n",
        "                    'n_estimators': [100, 200, 300],\n",
        "                    'max_depth': [3, 4, 6],\n",
        "                    'learning_rate': [0.01, 0.05, 0.1],\n",
        "                    'subsample': [0.8, 0.9],\n",
        "                    'colsample_bytree': [0.8, 0.9]\n",
        "                }\n",
        "\n",
        "                grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "                grid_search.fit(X_train, y_train)\n",
        "\n",
        "                print(f\"\\n{name} Hyperparameter Tuning Results:\")\n",
        "                print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "                best_model = grid_search.best_estimator_\n",
        "                self.models[name] = best_model\n",
        "\n",
        "                # Get predictions\n",
        "                train_pred = best_model.predict(X_train)\n",
        "                test_pred = best_model.predict(X_test)\n",
        "\n",
        "                # Print detailed reports\n",
        "                print_detailed_report(y_train, train_pred, \"Training\")\n",
        "                print_detailed_report(y_test, test_pred, \"Test\")\n",
        "\n",
        "            # Create and train ensemble\n",
        "            print(\"\\nTraining Ensemble Model...\")\n",
        "            ensemble = self.create_ensemble()\n",
        "            ensemble.fit(X_train, y_train)\n",
        "\n",
        "            # Evaluate ensemble\n",
        "            train_pred = ensemble.predict(X_train)\n",
        "            test_pred = ensemble.predict(X_test)\n",
        "\n",
        "            # Print ensemble results\n",
        "            print(\"\\nEnsemble Model Results:\")\n",
        "            print_detailed_report(y_train, train_pred, \"Training\")\n",
        "            print_detailed_report(y_test, test_pred, \"Test\")\n",
        "\n",
        "            # Plot confusion matrix\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            cm = confusion_matrix(y_test, test_pred)\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "            plt.title('Ensemble Model - Confusion Matrix (Test Set)')\n",
        "            plt.ylabel('True Label')\n",
        "            plt.xlabel('Predicted Label')\n",
        "            plt.savefig(f'confusion_matrix_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png')\n",
        "            plt.close()\n",
        "\n",
        "            # Save model\n",
        "            self.best_model = ensemble\n",
        "            model_filename = f'xgbrf_ensemble_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.joblib'\n",
        "            joblib.dump(ensemble, model_filename)\n",
        "\n",
        "            return ensemble\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in training and evaluation: {str(e)}\")\n",
        "            logging.error(\"Detailed error info:\", exc_info=True)\n",
        "            raise\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # Initialize classifier\n",
        "        classifier = URLEnsembleClassifier()\n",
        "\n",
        "        # Load and preprocess data\n",
        "        file_path = '/content/drive/MyDrive/Extracted CSV/converted_features.csv'  # Sesuaikan dengan path file Anda\n",
        "\n",
        "        logging.info(f\"Checking file at: {file_path}\")\n",
        "        if not os.path.exists(file_path):\n",
        "            logging.error(f\"File not found at: {file_path}\")\n",
        "            return\n",
        "\n",
        "        # Data preprocessing dengan min_samples=2\n",
        "        data = classifier.load_and_preprocess_data(file_path, sample_size=40000, min_samples=2)\n",
        "\n",
        "        if data is not None:\n",
        "            X_train, X_test, y_train, y_test, y_binary_train, y_binary_test = data\n",
        "            ensemble_model = classifier.train_and_evaluate(X_train, X_test, y_train, y_test)\n",
        "            logging.info(\"Training and evaluation completed successfully\")\n",
        "        else:\n",
        "            logging.error(\"Data preprocessing failed\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in main execution: {str(e)}\")\n",
        "        logging.error(\"Detailed error info:\", exc_info=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFhpog8Vv3S4",
        "outputId": "deaa2441-bfee-4187-927a-7cb8cf9d36c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AlkBfoAzG_3nw3dME_7aipCcRbAAwct8",
      "authorship_tag": "ABX9TyNZ4qQjXo5RhtzYAb8MYXRX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}